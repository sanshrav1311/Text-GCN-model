{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sanshrav1311/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import combinations\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(filename, data):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(data, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns); cols = [str(w) for w in cols]\n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('PubMedDataClean.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('Contextual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_graph(df, columnName):\n",
    "    word_set = set()\n",
    "    for doc_words in df[columnName]:\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "    vocab = list(word_set)\n",
    "    vocab_size = len(vocab)\n",
    "    corpus_size=8833\n",
    "    vocab_map = {}\n",
    "    for i in range(vocab_size):\n",
    "        vocab_map[vocab[i]] = i\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=nltk.word_tokenize,lowercase=False)\n",
    "    vectorizer.fit(df[columnName])\n",
    "    df_tfidf = vectorizer.transform(df[columnName])\n",
    "    df_tfidf = df_tfidf.toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    vocab = np.array(vocab)\n",
    "    df_tfidf = pd.DataFrame(df_tfidf,columns=vocab)\n",
    "    \n",
    "    names = vocab\n",
    "    window = 10\n",
    "    n_i  = OrderedDict((name, 0) for name in names)\n",
    "    word2index = OrderedDict( (name,index) for index,name in enumerate(names) )\n",
    "\n",
    "    occurrences = np.zeros( (len(names),len(names)) ,dtype=np.int32)\n",
    "    no_windows = 0; \n",
    "    for l in tqdm(df[columnName], total=len(df[columnName])):\n",
    "        k=nltk.word_tokenize(l)\n",
    "        for i in range(len(k)-window):\n",
    "            no_windows += 1\n",
    "            d = set(k[i:(i+window)])\n",
    "\n",
    "            for w in d:\n",
    "                n_i[w] += 1\n",
    "            for w1,w2 in combinations(d,2):\n",
    "                i1 = word2index[w1]\n",
    "                i2 = word2index[w2]\n",
    "\n",
    "                occurrences[i1][i2] += 1\n",
    "                occurrences[i2][i1] += 1\n",
    "    p_ij = pd.DataFrame(occurrences, index = names,columns=names)/no_windows\n",
    "    p_i = pd.Series(n_i, index=n_i.keys())/no_windows\n",
    "\n",
    "    del occurrences\n",
    "    del n_i\n",
    "    for col in p_ij.columns:\n",
    "        p_ij[col] = p_ij[col]/p_i[col]\n",
    "    for row in p_ij.index:\n",
    "        p_ij.loc[row,:] = p_ij.loc[row,:]/p_i[row]\n",
    "    p_ij = p_ij + 1E-9\n",
    "    for col in p_ij.columns:\n",
    "        p_ij[col] = p_ij[col].apply(lambda x: math.log(x))\n",
    "        \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df_tfidf.index) ## document nodes\n",
    "    G.add_nodes_from(vocab) ## word nodes\n",
    "    ### build edges between document-word pairs\n",
    "    document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index))\\\n",
    "                        for w in df_tfidf.columns]\n",
    "    word_word = word_word_edges(p_ij)\n",
    "    # save_as_pickle(\"word_word_edges_%s.pkl\" % columnName, word_word)\n",
    "    G.add_edges_from(document_word)\n",
    "    G.add_edges_from(word_word)\n",
    "    save_as_pickle(\"text_graph_%s.pkl\" % columnName, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 8833/8833 [00:04<00:00, 1972.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for column in [\"TITLE_CLEAN\", \"KEYWORDS_CLEAN\", \"ABSTRACT_CLEAN\"]:\n",
    "    generate_text_graph(df = df, columnName = column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
